{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "PATH_MAIN = '/project/ds5559/group2nba'\n",
    "PATH_STACKED = f'{PATH_MAIN}/stacked_data/'\n",
    "RESULTS_FILE = f'{PATH_MAIN}/results.csv'\n",
    "TARGET = 'Won'\n",
    "FEATURES = 'features'\n",
    "\n",
    "CORES = 4\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('group2nba') \\\n",
    "    .master(f'local[{CORES}]') \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from typing import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "FIELDS: Dict[str, T] = {\n",
    "      'Date': StringType\n",
    "    , 'HomeTeam': StringType\n",
    "    , 'AwayTeam': StringType\n",
    "    , 'Team': StringType\n",
    "    , 'Year': IntegerType\n",
    "    , 'Won': IntegerType\n",
    "    \n",
    "    , 'ScoreDiff': IntegerType\n",
    "    , 'Quarter': IntegerType\n",
    "    , 'SecLeftTotal': IntegerType\n",
    "    , 'LogSecLeftTotal': DoubleType\n",
    "    , 'SecLeftTotalInverse': DoubleType\n",
    "    \n",
    "    , 'HasPossession': IntegerType\n",
    "    , 'assist_team_cnt': LongType\n",
    "    , 'assist_opponent_cnt': LongType\n",
    "    , 'turnover_team_cnt': LongType\n",
    "    , 'turnover_opponent_cnt': LongType\n",
    "    , 'block_team_cnt': LongType\n",
    "    , 'block_opponent_cnt': LongType\n",
    "    \n",
    "    , 'foul_team_cnt': LongType\n",
    "    , 'foul_opponent_cnt': LongType\n",
    "    , 'rebound_team_cnt': LongType\n",
    "    , 'rebound_opponent_cnt': LongType\n",
    "    , 'shotOnGoal_team_cnt': LongType\n",
    "    , 'shotOnGoal_opponent_cnt': LongType\n",
    "    , 'freeThrow_team_cnt': LongType\n",
    "    , 'freeThrow_opponent_cnt': LongType\n",
    "    \n",
    "    , 'SecLeftTotalInverseTimesScoreDiff': DoubleType\n",
    "    , 'assist_diff': IntegerType\n",
    "    , 'turnover_diff': IntegerType\n",
    "    , 'block_diff': IntegerType\n",
    "    , 'foul_diff': IntegerType\n",
    "    , 'rebound_diff': IntegerType\n",
    "    , 'shotOnGoal_diff': IntegerType\n",
    "    , 'freeThrow_diff': IntegerType\n",
    "}\n",
    "    \n",
    "schema = StructType([StructField(k, v()) for k, v in FIELDS.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class results_cls:\n",
    "    model_type: str = ''\n",
    "    list_features: list = field(default_factory=list)\n",
    "    date_time_run: str = ''\n",
    "    user: str = ''\n",
    "    special_description: str = ''\n",
    "    cv_elapsed_time: str = ''\n",
    "    cv_area_under_roc: float = 0.0\n",
    "    cv_area_under_pr: float = 0.0\n",
    "    cv_best_coefficients: list = field(default_factory=list)\n",
    "    cv_best_hyperparameters: dict = field(default_factory=dict)\n",
    "    val_elapsed_time: str = ''\n",
    "    val_area_under_roc: float = 0.0\n",
    "    val_area_under_pr: float = 0.0\n",
    "    val_best_coefficients: list = field(default_factory=list)\n",
    "\n",
    "# results = results_cls(date_time_run = datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "# results.model_type='SVM'\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    logistic = 'Logistic'\n",
    "    random_forest = 'Random Forest'\n",
    "    \n",
    "    TARGET = 'Won'\n",
    "    FEATURES = 'features'\n",
    "    \n",
    "    def __init__(self, list_features, model_type):\n",
    "        from datetime import datetime\n",
    "        \n",
    "        # Defined in Methods\n",
    "        self.cvModel = None\n",
    "        self.cvPredictions = None\n",
    "        self.valModel = None\n",
    "        self.valPredictions = None\n",
    "        \n",
    "        print('Setup Model')\n",
    "        self.list_features = list_features\n",
    "        self.pipeline = self.build_pipeline()\n",
    "        self.model_type = model_type\n",
    "        self.evaluator = self.build_evaluator()\n",
    "        self.results = results_cls(\n",
    "            date_time_run = datetime.now().strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "        )\n",
    "        \n",
    "    def build_pipeline(self):\n",
    "        from pyspark.ml import feature as ft\n",
    "        from pyspark.ml import Pipeline\n",
    "\n",
    "        # Build the Pipeline\n",
    "        print('build the pipeline')\n",
    "\n",
    "        featuresCreator = ft.VectorAssembler(\n",
    "            inputCols=self.list_features,\n",
    "            outputCol='vectors'\n",
    "        )\n",
    "\n",
    "        sScaler = ft.StandardScaler(\n",
    "            withMean=True, \n",
    "            withStd=True, \n",
    "            inputCol='vectors', \n",
    "            outputCol='features'\n",
    "        )\n",
    "\n",
    "        pipeline = Pipeline(\n",
    "            stages=[\n",
    "                featuresCreator,\n",
    "                sScaler\n",
    "            ])\n",
    "\n",
    "        return pipeline\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_evaluator():\n",
    "        import pyspark.ml.evaluation as ev\n",
    "\n",
    "        evaluator = ev.BinaryClassificationEvaluator(\n",
    "            metricName = 'areaUnderROC',\n",
    "            rawPredictionCol='rawPrediction', \n",
    "            labelCol=TARGET\n",
    "        )\n",
    "        \n",
    "        return evaluator\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_hyperparams(cvModel):\n",
    "        import re\n",
    "\n",
    "        hyperparams = cvModel.getEstimatorParamMaps()[ np.argmax(cvModel.avgMetrics) ]\n",
    "        hyper_dict = {}\n",
    "\n",
    "        for i in range(len(hyperparams.items())):\n",
    "            hyper_name = re.search(\"name='(.+?)'\", str([x for x in hyperparams.items()][i])).group(1)\n",
    "            hyper_value = [x for x in hyperparams.items()][i][1]\n",
    "\n",
    "            hyper_dict[hyper_name]= hyper_value\n",
    "\n",
    "        print(hyper_dict)\n",
    "    \n",
    "        return hyper_dict\n",
    "    \n",
    "    def evaluate_cv_model(self, test_train_data):\n",
    "        import time\n",
    "\n",
    "        # Start the Timer\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_data, test_data = test_train_data.randomSplit([0.7, 0.3], seed=123) \n",
    "\n",
    "        # Fit the Model\n",
    "        print('Build Data Transformer')\n",
    "        data_transformer = self.pipeline.fit(train_data)\n",
    "\n",
    "        print('Transform Train Data + Fit CV Model')\n",
    "        cvModel = self.cvModel.setParallelism(CORES).fit(data_transformer.transform(train_data))\n",
    "\n",
    "        print('Transform Test Data')\n",
    "        data_train = data_transformer.transform(test_data)\n",
    "\n",
    "        print('Evaluate Model Against Test Data')\n",
    "        self.cv_predictions = cvModel.transform(data_train)\n",
    "\n",
    "        print('Store Results')\n",
    "        \n",
    "        self.results.cv_area_under_roc = self.evaluator.evaluate(\n",
    "            self.cv_predictions, \n",
    "            {self.evaluator.metricName: 'areaUnderROC'}\n",
    "        )\n",
    "        self.results.cv_area_under_pr = self.evaluator.evaluate(\n",
    "            self.cv_predictions, \n",
    "            {self.evaluator.metricName: 'areaUnderPR'}\n",
    "        )\n",
    "\n",
    "        # End Timer\n",
    "        self.results.cv_elapsed_time = round((time.time() - start_time), 2)\n",
    "\n",
    "        # Random Forest doesn't have coefficients\n",
    "        if self.model_type in ('Logistic', 'Support Vector Machine'):\n",
    "            self.results.cv_best_coefficients = cvModel.bestModel.coefficients\n",
    "            \n",
    "        self.results.best_hyperparameters = self.extract_hyperparams(cvModel)\n",
    "        \n",
    "    def evaluate_val_model(self, test_train_data, validation_data):\n",
    "        import time\n",
    "\n",
    "        # Start the Timer\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print('Build Data Transformer')\n",
    "        data_transformer = self.pipeline.fit(train_test_data)\n",
    "\n",
    "        print('Transform TestTrain Data + Fit Val Model')\n",
    "        valModel = self.valModel.fit(data_transformer.transform(train_test_data))\n",
    "        \n",
    "        print('Transform Validation Data')\n",
    "        data_train = data_transformer.transform(validation_data)\n",
    "\n",
    "        print('Evaluate Model Against Validation Data')\n",
    "        self.val_predictions = valModel.transform(data_train)\n",
    "        \n",
    "        print('Store Results')\n",
    "        \n",
    "        self.results.val_area_under_roc = self.evaluator.evaluate(\n",
    "            self.val_predictions, \n",
    "            {self.evaluator.metricName: 'areaUnderROC'}\n",
    "        )\n",
    "        self.results.val_area_under_pr = self.evaluator.evaluate(\n",
    "            self.val_predictions, \n",
    "            {self.evaluator.metricName: 'areaUnderPR'}\n",
    "        )\n",
    "\n",
    "        # End Timer\n",
    "        self.results.val_elapsed_time = round((time.time() - start_time), 2)\n",
    "\n",
    "        # Random Forest doesn't have coefficients\n",
    "        if self.model_type in ('Logistic', 'Support Vector Machine'):\n",
    "            self.results.val_best_coefficients = valModel.coefficients\n",
    "    \n",
    "    def save_results(results_obj):\n",
    "        from os.path import exists\n",
    "    \n",
    "        results_df = pd.DataFrame(data=[results_obj])\n",
    "        \n",
    "        file_exists = exists(RESULTS_FILE)\n",
    "        if file_exists:\n",
    "            results_df.to_csv(RESULTS_FILE, mode='a', index=False, header=False, sep=\"|\")\n",
    "        else:\n",
    "            results_df.to_csv(RESULTS_FILE, mode='w', index=False, header=True, sep=\"|\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMModel(Model):\n",
    "    SVM = 'Support Vector Machine'\n",
    "    \n",
    "    def __init__(self, list_features):\n",
    "        Model.__init__(\n",
    "            self, \n",
    "            list_features = list_features, \n",
    "            model_type = self.SVM\n",
    "        )\n",
    "        \n",
    "    def build_cv_model(self):\n",
    "        import pyspark.ml.evaluation as ev\n",
    "        from pyspark.ml.classification import LinearSVC\n",
    "        import pyspark.ml.tuning as tune\n",
    "\n",
    "        print('Build CVModel: SVM')\n",
    "\n",
    "        svm = LinearSVC(featuresCol = FEATURES, labelCol=TARGET)\n",
    "\n",
    "        grid = tune.ParamGridBuilder() \\\n",
    "                    .addGrid(svm.aggregationDepth, [3, 5, 10]) \\\n",
    "                    .addGrid(svm.maxIter, [10, 20, 50]) \\\n",
    "                    .build()\n",
    "\n",
    "        self.cvModel = tune.CrossValidator( \n",
    "            estimator=svm, \n",
    "            estimatorParamMaps=grid, \n",
    "            evaluator=self.evaluator,\n",
    "            numFolds=5\n",
    "        )\n",
    "    \n",
    "    def build_val_model(self, hyper_dict):\n",
    "        from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "        print('Build ValModel: SVM')\n",
    "\n",
    "        self.valModel = LinearSVC(featuresCol = FEATURES, labelCol=TARGET, **hyper_dict)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestModel(Model):\n",
    "    RANDOMFOREST = 'Random Forest'\n",
    "    \n",
    "    def __init__(self, list_features):\n",
    "        Model.__init__(\n",
    "            self, \n",
    "            list_features = list_features, \n",
    "            model_type = self.RANDOMFOREST\n",
    "        )\n",
    "        \n",
    "    def build_cv_model(self):\n",
    "        from pyspark.ml.classification import RandomForestClassifier\n",
    "        import pyspark.ml.tuning as tune\n",
    "\n",
    "        print('Build CVModel: Random Forest')\n",
    "\n",
    "        random_forest = RandomForestClassifier(featuresCol = FEATURES, labelCol=TARGET)\n",
    "\n",
    "        grid = tune.ParamGridBuilder() \\\n",
    "            .addGrid(random_forest.maxBins, [2, 3]) \\\n",
    "            .addGrid(random_forest.maxDepth, [3, 5]) \\\n",
    "            .addGrid(random_forest.numTrees, [100, 500]) \\\n",
    "            .build()\n",
    "\n",
    "        self.cvModel = tune.CrossValidator( \n",
    "            estimator=random_forest, \n",
    "            estimatorParamMaps=grid, \n",
    "            evaluator=self.evaluator,\n",
    "            numFolds=5\n",
    "        )\n",
    "    \n",
    "    def build_val_model(self, hyper_dict):\n",
    "        from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "        print('Build ValModel: Random Forest')\n",
    "\n",
    "        self.valModel = RandomForestClassifier(featuresCol = FEATURES, labelCol=TARGET, **hyper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticModel(Model):\n",
    "    LOGISTIC = 'Logistic'\n",
    "    \n",
    "    def __init__(self, list_features):\n",
    "        Model.__init__(\n",
    "            self, \n",
    "            list_features = list_features, \n",
    "            model_type = self.RANDOMFOREST\n",
    "        )\n",
    "        \n",
    "    def build_cv_model(self):\n",
    "        from pyspark.ml.classification import LogisticRegression\n",
    "        import pyspark.ml.tuning as tune\n",
    "\n",
    "        print('Build CVModel: Logistic Regression')\n",
    "\n",
    "        logistic = LogisticRegression(featuresCol = FEATURES, labelCol=TARGET)\n",
    "\n",
    "        grid = tune.ParamGridBuilder() \\\n",
    "            .addGrid(pipeline.getStages()[1].maxIter, [10, 20]) \\\n",
    "            .addGrid(pipeline.getStages()[1].regParam, [0.1, 0.5]) \\\n",
    "            .build()\n",
    "\n",
    "        self.cvModel = tune.CrossValidator( \n",
    "            estimator=logistic, \n",
    "            estimatorParamMaps=grid, \n",
    "            evaluator=self.evaluator,\n",
    "            numFolds=5\n",
    "        )\n",
    "    \n",
    "    def build_val_model(self, hyper_dict):\n",
    "        from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "        print('Build ValModel: Logistic Regression')\n",
    "\n",
    "        self.valModel = LogisticRegression(featuresCol = FEATURES, labelCol=TARGET, **hyper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_file(full_file_name):\n",
    "\n",
    "    df = spark.read \\\n",
    "        .format('csv') \\\n",
    "        .option('header', True) \\\n",
    "        .schema(schema) \\\n",
    "        .load(full_file_name)\n",
    "\n",
    "#     display(df.count())\n",
    "#     display(df.printSchema())\n",
    "#     display(df.head(2))\n",
    "    \n",
    "    return df\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "test_train = 'NBA_PBP_2018-19.csv'\n",
    "validation = 'NBA_PBP_2019-20.csv'\n",
    "\n",
    "# Load in your data - Can append more files to test_train if you want\n",
    "#   by extending the 'read_in_files' with ',' between full file names\n",
    "test_train_df = read_in_file(join(PATH_STACKED, test_train))\n",
    "validation_df = read_in_file(join(PATH_STACKED, validation))\n",
    "\n",
    "# Any Modifications to the Data you want to make\n",
    "test_train_df = test_train_df.where(F.col('SecLeftTotal') <= 300)\n",
    "validation_df = validation_df.where(F.col('SecLeftTotal') <= 300)\n",
    "\n",
    "# LogSecLeftTotal\n",
    "list_features = ['ScoreDiff', 'SecLeftTotalInverse', 'SecLeftTotalInverseTimesScoreDiff']\n",
    "    \n",
    "# test_model = SVMModel(list_features, test_train_df, validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attach Data\n",
      "build the pipeline\n"
     ]
    }
   ],
   "source": [
    "test_model = SVMModel(list_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build CVModel: SVM\n",
      "Build Pipeline\n",
      "Fit CV Model\n"
     ]
    }
   ],
   "source": [
    "test_model.build_cv_model()\n",
    "test_model.evaluate_cv_model(test_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.build_val_model()\n",
    "test_model.evaluate_val_model(test_train_df, validation_df)\n",
    "\n",
    "test_model.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>list_features</th>\n",
       "      <th>date_time_run</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>user</th>\n",
       "      <th>special_description</th>\n",
       "      <th>area_under_roc</th>\n",
       "      <th>area_under_pr</th>\n",
       "      <th>best_coefficients</th>\n",
       "      <th>best_hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>['ScoreDiff', 'SecLeftTotalInverse', 'SecLeftT...</td>\n",
       "      <td>04/17/2022 23:35:57</td>\n",
       "      <td>309.49</td>\n",
       "      <td>Peter</td>\n",
       "      <td>Test Run 19-20 Only</td>\n",
       "      <td>0.820474</td>\n",
       "      <td>0.823240</td>\n",
       "      <td>[1.4749755253169836,-9.339839834844648e-05,0.4...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>['ScoreDiff', 'SecLeftTotalInverse', 'SecLeftT...</td>\n",
       "      <td>04/17/2022 23:49:30</td>\n",
       "      <td>306.11</td>\n",
       "      <td>Peter</td>\n",
       "      <td>Test Run 19-20 Only</td>\n",
       "      <td>0.820472</td>\n",
       "      <td>0.823239</td>\n",
       "      <td>[1.4749598979624223,-9.384355132570414e-05,0.4...</td>\n",
       "      <td>{Param(parent='LogisticRegression_b762bb9a2c14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>['ScoreDiff', 'SecLeftTotalInverse', 'SecLeftT...</td>\n",
       "      <td>04/19/2022 11:26:57</td>\n",
       "      <td>151.95</td>\n",
       "      <td>Peter</td>\n",
       "      <td>Test Run 19-20 Only</td>\n",
       "      <td>0.820471</td>\n",
       "      <td>0.823240</td>\n",
       "      <td>[1.4749598979624223,-9.384355132568476e-05,0.4...</td>\n",
       "      <td>{Param(parent='LogisticRegression_5e121c16d293...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>['ScoreDiff', 'SecLeftTotalInverse', 'SecLeftT...</td>\n",
       "      <td>04/19/2022 15:31:11</td>\n",
       "      <td>444.19</td>\n",
       "      <td>Peter</td>\n",
       "      <td>Test Run 19-20 Only</td>\n",
       "      <td>0.827061</td>\n",
       "      <td>0.833835</td>\n",
       "      <td>[1.2087398363120254,0.04048200887116883,21.764...</td>\n",
       "      <td>{Param(parent='LinearSVC_fc987ac4b343', name='...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_type                                      list_features  \\\n",
       "0                Logistic  ['ScoreDiff', 'SecLeftTotalInverse', 'SecLeftT...   \n",
       "1                Logistic  ['ScoreDiff', 'SecLeftTotalInverse', 'SecLeftT...   \n",
       "2                Logistic  ['ScoreDiff', 'SecLeftTotalInverse', 'SecLeftT...   \n",
       "3  Support Vector Machine  ['ScoreDiff', 'SecLeftTotalInverse', 'SecLeftT...   \n",
       "\n",
       "         date_time_run  elapsed_time   user  special_description  \\\n",
       "0  04/17/2022 23:35:57        309.49  Peter  Test Run 19-20 Only   \n",
       "1  04/17/2022 23:49:30        306.11  Peter  Test Run 19-20 Only   \n",
       "2  04/19/2022 11:26:57        151.95  Peter  Test Run 19-20 Only   \n",
       "3  04/19/2022 15:31:11        444.19  Peter  Test Run 19-20 Only   \n",
       "\n",
       "   area_under_roc  area_under_pr  \\\n",
       "0        0.820474       0.823240   \n",
       "1        0.820472       0.823239   \n",
       "2        0.820471       0.823240   \n",
       "3        0.827061       0.833835   \n",
       "\n",
       "                                   best_coefficients  \\\n",
       "0  [1.4749755253169836,-9.339839834844648e-05,0.4...   \n",
       "1  [1.4749598979624223,-9.384355132570414e-05,0.4...   \n",
       "2  [1.4749598979624223,-9.384355132568476e-05,0.4...   \n",
       "3  [1.2087398363120254,0.04048200887116883,21.764...   \n",
       "\n",
       "                                best_hyperparameters  \n",
       "0                                                 []  \n",
       "1  {Param(parent='LogisticRegression_b762bb9a2c14...  \n",
       "2  {Param(parent='LogisticRegression_5e121c16d293...  \n",
       "3  {Param(parent='LinearSVC_fc987ac4b343', name='...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_df = pd.read_csv(RESULTS_FILE, sep='|')\n",
    "view_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/ds5559/group2nba/stacked_data/NBA_PBP_2015-16.csv,/project/ds5559/group2nba/stacked_data/NBA_PBP_2016-17.csv,/project/ds5559/group2nba/stacked_data/NBA_PBP_2017-18.csv,/project/ds5559/group2nba/stacked_data/NBA_PBP_2018-29.csv\n"
     ]
    }
   ],
   "source": [
    "train_files = [\n",
    "    'NBA_PBP_2015-16.csv',\n",
    "    'NBA_PBP_2016-17.csv',\n",
    "    'NBA_PBP_2017-18.csv',\n",
    "    'NBA_PBP_2018-29.csv'\n",
    "]\n",
    "\n",
    "validation_file = 'NBA_PBP_2019-20.csv'\n",
    "\n",
    "# Read in all Train_Files\n",
    "\n",
    "file_str = ''\n",
    "for item in train_files:\n",
    "    file_str = file_str + PATH_STACKED + item + ','\n",
    "    \n",
    "file_str = file_str[:-1]\n",
    "print(file_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
